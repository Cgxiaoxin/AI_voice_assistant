# 项目梳理文档

## 1. 项目概述

该项目是一个基于语音交互的控制系统，核心功能包括：

1.  **语音唤醒**：通过特定的唤醒词激活系统。
2.  **语音识别 (ASR)**：将用户的语音指令转换成文本。
3.  **自然语言理解 (NLU)**：提取转换后文本中的关键词，并匹配预定义的指令。
4.  **设备控制**：通过 Modbus TCP 协议与外部设备（可能是机器人或自动化装置）通信，发送控制指令。
5.  **语音合成 (TTS)**：将系统的反馈信息合成为语音进行播报。
6.  **状态/表情显示**：通过一个 WebSocket 服务器，向连接的客户端（如网页界面）发送指令，用于显示设备的状态或模拟表情。

项目主要依赖科大讯飞的语音服务（唤醒、识别、合成）和 `pymodbus` 库进行 Modbus 通信，以及 `jieba` 库进行中文分词。

## 2. 主要文件分析

### 2.1. `new_all.py` (核心业务逻辑)

*   **主要功能**: 这是项目的主程序。它集成了语音唤醒、语音识别、指令解析、通过 Modbus 控制硬件，以及与 `ExpressionServer`（表情服务）的交互。
*   **核心类 `WakeWordJieba`**:
    *   `__init__(self)`:
        *   初始化 Modbus 服务器配置 (默认 IP: `192.168.1.125`, Port: `599`) 和数据存储。
        *   创建必要的目录 (`keywords/`, `logs/`, `keywords_dicts/`)。
        *   配置 `loguru` 日志系统。
        *   加载关键词词典 (`keywords_dicts/keywords_dict.yaml`)。
        *   初始化科大讯飞语音唤醒引擎 (使用 `./bin/msc_x64.dll` 和相关资源)。科大讯飞相关配置 (APPID, APIKey, APISecret) 在文件开头定义。
        *   初始化 `ExpressionServer` (来自 `change_state.py`)并启动它。
    *   `setup_logger(logs_dir)`: (全局函数) 配置日志记录器，日志会按日期和级别（INFO, ERROR）保存在 `logs/` 目录下。
    *   `load_keywords_dict(self)`: 从 `keywords_dicts/keywords_dict.yaml` 文件加载关键词配置，用于后续指令匹配。
    *   `wake_callback(self, sessionID, msg, param1, param2, info, userDate)`: 科大讯飞唤醒引擎的回调函数。当检测到唤醒词时被调用，会通过 TTS (调用 `text_to_speech`) 说出问候语，并通过 `ExpressionServer` 发送指令（如眨眼、显示文字）。
    *   `request_monitor(self)` (async): 异步任务，监控 Modbus 服务器的保持寄存器。如果特定地址的值变为 `111`，会记录日志、尝试清零该值，并设置退出事件以尝试停止程序。
    *   `run_modbus_server(self)` (async): 启动 Modbus TCP 从站服务器，并运行 `request_monitor`。
    *   `identity(self)`: 返回 Modbus 设备的标识信息。
    *   `start_wake_detection(self)`: 启动科大讯飞的唤醒词检测流程，包括MSP登录、设置参数、注册回调、打开麦克风音频流并持续写入音频数据进行检测。
    *   `start_speech_recognition(self)`: 在唤醒成功后，启动科大讯飞的语音识别（实时语音转写）流程。
    *   `create_url(self)`: 为科大讯飞语音识别服务创建 WebSocket 连接 URL，包含鉴权信息。
    *   `extract_keywords(self, text, topK=5)`: 使用 `jieba.analyse.extract_tags` 从识别到的文本中提取关键词。
    *   `save_results_to_yaml(self, text, keywords)`: 将原始识别文本和提取的关键词保存到 `keywords/` 目录下的 YAML 文件中，文件名包含时间戳。
    *   `on_message(self, ws, message)`: WebSocket 回调，处理科大讯飞语音识别服务返回的实时结果，拼接识别文本。
    *   `on_error(self, ws, error)`: WebSocket 错误回调。
    *   `process_text_and_send_signal(self, text)`: **核心指令处理逻辑**。
        *   将识别到的文本发送给 `ExpressionServer` 显示 (如 `self.expressions_server.send("text_person", text)`)。
        *   根据关键词（如"放"、"拿"、"抓"、"停"等）和分词结果（使用 `jieba`），解析用户意图。
        *   使用 `match_keywords` 方法，将分词结果与 `keywords_dict.yaml` 中定义的物品和位置进行模糊匹配（使用 `difflib.SequenceMatcher`）。
        *   根据匹配成功的组合，通过 `self.store.setValues(3, 1, [CODE])` 修改 Modbus 保持寄存器的值，从而向外部设备发送指令（例如，401, 402, 403, 404 代表不同的放置操作，405 代表停止，406 代表无法识别或不支持的功能）。
        *   通过 `ExpressionServer` 发送指令更新表情/状态 (如 `self.expressions_server.send("img", "happy")`)。
        *   通过 `text_to_speech` (来自 `request.py`) 给出语音反馈。
    *   `match_keywords(self, extracted_keywords, items)`: 辅助函数，将提取的关键词与词典中定义的物品列表进行匹配，返回匹配度最高的物品名称。
    *   `on_close(self, ws, close_status_code, close_msg)`: WebSocket 关闭回调。语音识别结束后，调用 `process_text_and_send_signal` 处理最终识别的完整文本。
    *   `on_open(self, ws)`: WebSocket 打开回调。启动一个新线程 (`_thread.start_new_thread`)，负责从麦克风读取音频数据，编码后发送给科大讯飞语音识别服务。
*   `main()` (async 全局函数):
    *   创建 `WakeWordJieba` 实例。
    *   创建线程池 (`ThreadPoolExecutor`)。
    *   首先异步运行 `run_modbus_server`。
    *   进入主循环，在线程池中执行 `detector.start_wake_detection`。如果唤醒成功 (`detector.is_awake` 为 True)，则在线程池中执行 `detector.start_speech_recognition`。
    *   循环监听唤醒词，直到程序被中断 (如 `KeyboardInterrupt`) 或 `exit_event` 被设置。

### 2.2. `all.py` (旧版或备用核心逻辑)

*   **主要功能**: 与 `new_all.py` 非常相似，包含了大部分语音唤醒、识别、Modbus 控制的逻辑。
*   **主要区别**:
    *   没有集成 `ExpressionServer` (`change_state.py`)。
    *   Modbus IP 地址可能不同 (在 `all.py` 中是 `192.168.1.138`，而 `new_all.py` 中是 `192.168.1.125`)。
    *   语音录制参数 `RECORD_SECONDS` 可能不同。
    *   `process_text_and_send_signal` 的具体指令解析逻辑和 `match_keywords` 的使用方式可能略有差异，但目标一致：解析指令并发送Modbus信号。

### 2.3. `request.py` (文本转语音模块)

*   **主要功能**: 调用科大讯飞的语音合成 (TTS) 服务，将文本转换为语音并播放。
*   **注意**: 此文件使用了与 `new_all.py`/`all.py` 中 ASR/IVW 服务不同的科大讯飞 `APPID`, `APIKey`, `APISecret` (定义在此文件开头)。
*   **主要函数**:
    *   `create_url()`: 为科大讯飞语音合成服务创建 WebSocket 连接 URL，包含鉴权信息。
    *   `on_message(ws, message)`: WebSocket 回调，处理科大讯飞 TTS 服务返回的音频流数据，解码 (base64) 并存入缓冲区 `ws.audio_data`。
    *   `play_audio(audio_data)`: 使用 `pyaudio` 播放完整的音频数据。
    *   `on_error(ws, error)`: WebSocket 错误回调。
    *   `on_close(ws, close_status_code, close_msg)`: WebSocket 关闭回调。当所有音频数据接收完毕后，调用 `play_audio` 播放。
    *   `on_open(ws, text)`: WebSocket 打开回调。启动一个新线程 (`threading.Thread`)，发送包含待合成文本 (base64编码后) 的请求给 TTS 服务。可以选择不同的发音人（如 `xiaoyan`）。
    *   `text_to_speech(text)`: **主调用函数**。接收文本字符串，建立到科大讯飞 TTS 服务的 WebSocket 连接，并处理整个语音合成与播放流程。

### 2.4. `change_state.py` (表情/状态显示 WebSocket 服务器)

*   **主要功能**: 创建一个 WebSocket 服务器，用于接收来自主程序 (`new_all.py`) 的指令，并将这些指令（通常是JSON格式的消息，包含类型如 "img", "text_robot", "text_person", "text_start" 和具体内容）广播给所有连接的 WebSocket 客户端。这可以用于驱动一个外部界面显示机器人的表情、对话文本或其他状态信息。
*   **核心类 `ExpressionServer`**:
    *   `__init__(self, host="0.0.0.0", port=8888)`: 初始化服务器，默认监听所有网络接口的 `8888` 端口。
    *   `_run_server(self)`: 内部方法，在独立的 asyncio 事件循环中启动 WebSocket 服务器和（可能存在的）定时任务。
    *   `_start_server(self)` (async): 启动 `websockets` 服务器。
    *   `_client_handler(self, websocket, path)` (async): 处理每个连接的客户端，将其加入客户端集合 (`self.clients`)，并保持连接等待消息。
    *   `start(self)`: **启动服务器**。在一个新的守护线程中运行 `_run_server`。
    *   `send(self, message_type: str, content: str)`: **向客户端发送消息**。
        *   构造包含 `type` 和 `content` 的 JSON 消息。
        *   使用 `asyncio.run_coroutine_threadsafe` 将异步的 `_broadcast` 方法安全地提交到服务器的事件循环中执行。
    *   `_broadcast(self, message: str)` (async): 异步方法，将消息发送给所有当前连接且处于打开状态的客户端。
    *   `_get_actual_ip(self)`: 尝试获取服务器实际绑定的 IP 地址（通过连接外部IP）。
    *   `stop(self)`: 停止服务器，关闭事件循环。
    *   `_send_periodically(self)` (async): 一个定时任务，如果 `self.message_data["type"] == "img"`，则会以 `self.timer_inierval` (0.3秒) 的间隔周期性地重新发送 `self.message_data` 中的 "img" 类型消息。
    *   `if __name__ == "__main__":` 部分提供了一个简单的命令行交互示例，用于通过控制台输入来测试服务器的 `send` 功能，主要用于发送表情指令。

## 3. 运行逻辑流程 (以 `new_all.py` 为例)

1.  **启动**: 执行 `python new_all.py`。
2.  **初始化**:
    *   `WakeWordJieba` 对象被创建。
    *   日志系统配置完成。
    *   Modbus TCP 从站服务器在 `192.168.1.125:599` (或配置文件中指定的IP/端口) 启动，并开始通过 `request_monitor` 监控特定寄存器。
    *   `ExpressionServer` 在 `0.0.0.0:8888` (或 `change_state.py` 中配置的地址/端口) 启动，等待客户端连接。
    *   科大讯飞唤醒引擎初始化。
3.  **等待唤醒**:
    *   程序进入 `start_wake_detection`，通过麦克风监听唤醒词。该唤醒词由科大讯飞开放平台配置和项目中的 `wakeupresource.jet` 文件共同决定。
4.  **唤醒成功**:
    *   `wake_callback` 被触发。
    *   系统通过 `text_to_speech` (调用 `request.py` 中的功能) 播报问候语 (例如 "你好，需要我为你做什么")。
    *   通过 `ExpressionServer` 发送指令（如 `self.expressions_server.send("img", "blink")` 使表情眨眼，`send("text_robot", ...)` 显示机器人说话内容, `send("text_start", "start")` 触发开始信号）。
    *   设置 `self.is_awake = True`。
5.  **语音识别**:
    *   程序调用 `start_speech_recognition`。
    *   与科大讯飞的语音识别 WebSocket 服务建立连接。
    *   实时录音并通过 WebSocket 发送音频流。
    *   `on_message` 回调接收并拼接识别出的文本片段，存入 `self.text_buffer`。
6.  **指令处理**:
    *   语音识别结束 (WebSocket 关闭) 后，`on_close` 回调被触发。
    *   调用 `process_text_and_send_signal` 处理 `self.text_buffer` 中的完整指令文本。
    *   **文本解析**: 使用 `jieba` 分词，并结合预定义的动词（如"放"、"拿"、"抓"、"停"）和名词（通过 `match_keywords` 与 `keywords_dict.yaml` 匹配具体物品和目标位置）。
    *   **发送Modbus指令**: 根据解析结果，向 Modbus 服务器的特定保持寄存器 (`slave_id=0`, `function_code=3`, `address=1`) 写入对应的状态码 (如 401, 402, 403, 404, 405, 406)。
    *   **表情/状态更新**: 通过 `ExpressionServer` 发送指令更新显示界面上的内容。
    *   **语音反馈**: 通过 `text_to_speech` 给出操作反馈或错误提示。
7.  **重置并继续**:
    *   `self.is_awake` 重置为 `False`。
    *   程序返回步骤 3，继续等待下一次唤醒。
8.  **Modbus 外部触发 (并行任务)**:
    *   `request_monitor` 持续在后台运行。如果检测到 Modbus 保持寄存器 (从地址0开始的10个值中) 包含 `111`，则会记录日志，尝试将该保持寄存器的第一个值设为0，并设置 `self.exit_event` 以尝试优雅地停止主循环和Modbus服务器。

## 4. 配置文件和重要目录

*   **`keywords_dicts/keywords_dict.yaml`**: 存放关键词与物品/指令的映射关系。用于自然语言理解，将用户的口语化指令映射到具体的控制目标。
    *   示例结构:
        ```yaml
        items:
          - name: 'item1'  # 例如，代表"绿色方块"
            keywords: ['绿色方块', '绿方块', '绿色块', '绿色']
          - name: 'item2'  # 例如，代表"红色圆形"
            keywords: ['红色圆形', '红圆', '红色圈', '红色']
          - name: 'item3'  # 例如，代表"方形盒子"
            keywords: ['方形', '方盒', '方的', '方形盒子']
          - name: 'item4'  # 例如，代表"圆形盒子"
            keywords: ['圆形', '圆盒', '圆的', '圆形盒子']
          - name: 'item5'  # 例如，代表停止相关的词
            keywords: ['停止', '停下', '停']
        ```
*   **`bin/`**: 存放运行所需的二进制文件和科大讯飞SDK相关的资源。
    *   `msc_x64.dll`: 科大讯飞语音服务的核心动态链接库 (Windows 64位)。
    *   `msc/res/ivw/wakeupresource.jet`: 科大讯飞唤醒引擎的资源文件，其中包含了唤醒词的模型。
*   **`logs/`**: 存放程序运行产生的日志文件。日志文件会根据配置按日期和级别（info, error）自动分割和管理。
*   **`keywords/`**: 语音识别完成后，程序会将原始识别文本和提取的关键词以 YAML 格式保存在此目录下，文件名通常包含时间戳，便于追踪和分析。

## 5. 主要依赖库

*   `pyaudio`: 用于录音和播放音频。
*   `loguru`: 提供易用且功能强大的日志记录。
*   `websocket-client`: 用于创建 WebSocket 客户端，与科大讯飞的 ASR 和 TTS 服务通信。
*   `websockets`: 用于创建 WebSocket 服务器 (在 `change_state.py` 中使用)。
*   `jieba`: 中文分词库，用于从识别的文本中提取关键词。
*   `PyYAML`: 用于读写 YAML 格式的配置文件 (如 `keywords_dict.yaml`)。
*   `pymodbus`: Modbus 通信协议库，用于与外部设备进行数据交换。
*   `asyncio`: Python 的异步I/O框架，`pymodbus` 和 `websockets` 服务器部分有使用。
*   `ctypes`: 用于加载 C 语言编写的动态链接库 (如 `msc_x64.dll`)。

## 6. 注意事项与配置

*   **科大讯飞凭证**:
    *   `new_all.py` (或 `all.py`) 文件顶部的 `APPID`, `APIKey`, `APISecret` 用于**语音唤醒 (ivw) 和语音识别 (iat)**。
    *   `request.py` 文件顶部的 `APPID`, `APIKey`, `APISecret` 用于**语音合成 (tts)**。
    *   这两套凭证需要从科大讯飞开放平台分别申请并正确配置。
*   **Modbus 地址**:
    *   `new_all.py` (或 `all.py`) 中的 `self.modbus_ip` 和 `self.modbus_port` 需要根据实际连接的 Modbus 从站设备进行配置。
*   **网络环境**:
    *   程序需要能够访问互联网以下载依赖库和连接科大讯飞的云服务。
    *   Modbus 通信和 `ExpressionServer` 的 WebSocket 通信需要在局域网内正确配置IP地址和端口，并确保防火墙允许相应的通信。
*   **唤醒词**: 唤醒词本身是在科大讯飞开放平台配置并下载到 `wakeupresource.jet` 文件中的，程序代码中只负责加载此资源。
*   **Python 版本**: 项目中使用了 f-string、async/await 等特性，建议使用 Python 3.7+。
*   **DLL 路径**: `self.msc_load_library = './bin/msc_x64.dll'` 路径需要正确。
*   **`ExpressionServer` 客户端**: 需要一个 WebSocket 客户端来连接 `ExpressionServer` (默认 `ws://<服务器IP>:8888`) 以接收并显示表情/文本信息。 